{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI for Linear Regression Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of datapoints\n",
    "n = 11\n",
    "\n",
    "# evenly spaced features\n",
    "x_train = torch.linspace(0,1,n)\n",
    "\n",
    "# design matrix\n",
    "Phi = torch.vstack((torch.ones(n),x_train)).T\n",
    "\n",
    "# \"true\" parameter values\n",
    "W_true = torch.ones(2,1)\n",
    "\n",
    "# Observational noise\n",
    "sigma2_obs = 1e-2\n",
    "\n",
    "# Compute the true y and corrupt with noise\n",
    "y_train = Phi @ W_true + torch.randn(n,1)*np.sqrt(sigma2_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to specify prior distributions for model parameters.  Good choices would be mean zero with a large variance (i.e. a vague prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify priors\n",
    "mu_prior = torch.zeros(2,1)\n",
    "sigma2_prior = torch.ones(2,1)\n",
    "sigma2_prior.data[:]*=100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll initialize the parameters of our variational distribution.  We'll assume the mean-field approximation over both the slope and intercept with a normal variational posterior with learnable mean and standard deviation\n",
    "$$\n",
    "q(\\mathbf{w}) = \\mathcal{N}(w_0;\\mu_0,\\sigma_0) \\mathcal{N}(w_1;\\mu_1,\\sigma_1).\n",
    "$$\n",
    "Note the parameterization in terms of std. dev. instead of variance.  This avoids having to explicitly constrain the variable to be positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_post = torch.rand(2,1,requires_grad=True)\n",
    "sigma_post = torch.rand(2,1,requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will optimize the loss\n",
    "$$\n",
    "L = \\frac{1}{m} \\sum_{j=1}^m \\log P(\\mathbf{y}_{obs} | \\mathbf{x}_{obs}, \\mathbf{w}_j) + KL(q(\\mathbf{w}) || p(\\mathbf{w}) ),\n",
    "$$\n",
    "with $\\mathbf{w}_j \\sim q(\\mathbf{w})$ and $p(\\mathbf{w})$ the prior defined above.  Thus we need a log-likelihood and a function that computes the (analytical) KL distribution between two diagonal normals.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(y_pred,y_obs,sigma2_obs):\n",
    "    return n/2*np.sqrt(2*np.pi*sigma2_obs) + 0.5/sigma2_obs*torch.sum((y_pred - y_obs)**2)\n",
    "\n",
    "def kl_gaussian_diagonal(mu_post,mu_prior,sigma2_post,sigma2_prior):\n",
    "    k = len(mu_post)\n",
    "    log_det_ratio = torch.log(sigma2_prior).sum() - torch.log(sigma2_post).sum()\n",
    "    sse = torch.sum((mu_post - mu_prior)**2/sigma2_prior)\n",
    "    trace = (sigma2_post/sigma2_prior).sum()\n",
    "    return 0.5*(log_det_ratio - k + sse + trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform a training loop with simple stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 2000\n",
    "\n",
    "# Number of likelihood evaluations for MC-approximation to data loss\n",
    "n_samples = 4\n",
    "\n",
    "# step size (this is annoying to tune: better to use ADAM)\n",
    "eta = 1e-5\n",
    "\n",
    "data_losses = []\n",
    "kl_losses = []\n",
    "\n",
    "# training loop\n",
    "for i in range(n_iterations):\n",
    "  \n",
    "\n",
    "    # Compute data loss\n",
    "    L_data = 0\n",
    "    for j in range(n_samples):\n",
    "        # Sample parameters\n",
    "        W = mu_post + torch.randn(2,1)*sigma_post\n",
    "        \n",
    "        # Compute forward model\n",
    "        y_pred = Phi @ W\n",
    "        \n",
    "        # Update data loss\n",
    "        L_data = L_data + log_likelihood(y_pred,y_train,sigma2_obs)/n_samples\n",
    "    \n",
    "    \n",
    "    # Compute KL-loss (prior)\n",
    "    L_kl = kl_gaussian_diagonal(mu_post,mu_prior,sigma_post**2,sigma2_prior)\n",
    "    \n",
    "    L = L_data + L_kl\n",
    "    L.backward()\n",
    "    \n",
    "    data_losses.append(L_data.detach())\n",
    "    kl_losses.append(L_kl.detach())\n",
    "    \n",
    "    # Gradient descent\n",
    "    with torch.no_grad():\n",
    "        mu_post -= eta*mu_post.grad\n",
    "        sigma_post -= eta*sigma_post.grad\n",
    "    mu_post.grad = None\n",
    "    sigma_post.grad = None \n",
    "    \n",
    "# Plot convergence \n",
    "plt.semilogy(data_losses)\n",
    "plt.semilogy(kl_losses)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian linear regression has an analytical solution for both posterior mean and (full) covariance matrix.  This is useful for comparison.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior precision\n",
    "Sigma_inv = 1./sigma2_obs*Phi.T @ Phi + torch.diag(1./sigma2_prior)\n",
    "\n",
    "# Posterior covariance\n",
    "Sigma = torch.linalg.inv(Sigma_inv)\n",
    "\n",
    "# Posterior cholesky decomp (matrix square root)\n",
    "L = torch.linalg.cholesky(Sigma)\n",
    "\n",
    "# Posterior mean\n",
    "mu_analytical = 1./sigma2_obs * Sigma @ Phi.T @ y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize both the posterior predictive distribution (the set of lines that are consistent with the data) as well as the posterior distribution (the slope and intercept samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Number of posterior samples to draw\n",
    "    n_plots = 500\n",
    "    fig,axs = plt.subplots(nrows=2)\n",
    "    for i in range(n_plots):\n",
    "        # Random sample from variational posterior\n",
    "        W = mu_post + torch.randn(2,1)*sigma_post\n",
    "        y_pred = Phi @ W \n",
    "        # Plot predictive\n",
    "        axs[0].plot(x_train,y_pred.detach(),'k-',alpha=0.1)\n",
    "        # Plot parameters\n",
    "        axs[1].plot(W[0],W[1],'k.')\n",
    "        \n",
    "        # Bonus plot: samples drawn from the analytical posterior\n",
    "        W_analytical = mu_analytical + L @ torch.randn(2,1)\n",
    "        axs[1].plot(W_analytical[0],W_analytical[1],'g.',alpha=0.2)\n",
    "        \n",
    "    # Plot data/true values\n",
    "    axs[0].plot(x_train,y_train,'ro')\n",
    "    axs[1].plot(W_true[0],W_true[1],'ro')\n",
    "fig.set_size_inches(6,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
